import asyncio
import json
import logging
import time
from contextlib import asynccontextmanager
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import aiohttp

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class FetchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è HTTP-–∑–∞–ø—Ä–æ—Å–∞"""

    url: str
    success: bool
    content: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    response_time: float = 0.0


class URLFetcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ URL"""

    def __init__(
        self,
        max_concurrent: int = 5,
        timeout: aiohttp.ClientTimeout = aiohttp.ClientTimeout(total=30),
        max_retries: int = 2,
    ):
        """
        Args:
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        """
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.max_retries = max_retries
        self.semaphore = asyncio.Semaphore(max_concurrent)

    @asynccontextmanager
    async def _create_session(self) -> aiohttp.ClientSession:
        """–°–æ–∑–¥–∞–µ—Ç –∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç —Å–µ—Å—Å–∏–µ–π aiohttp"""
        connector = aiohttp.TCPConnector(limit=self.max_concurrent, limit_per_host=2)
        session = aiohttp.ClientSession(
            connector=connector,
            timeout=self.timeout,
            headers={"User-Agent": "AsyncURLFetcher/1.0"},
        )
        try:
            yield session
        finally:
            await session.close()

    async def _fetch_single_url(
        self, session: aiohttp.ClientSession, url: str
    ) -> FetchResult:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω HTTP-–∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        start_time = time.time()

        for attempt in range(self.max_retries + 1):
            try:
                async with self.semaphore:
                    async with session.get(url) as response:
                        if response.status == 200:
                            # –ß–∏—Ç–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º JSON
                            text = await response.text()
                            try:
                                json_content = json.loads(text)
                                return FetchResult(
                                    url=url,
                                    success=True,
                                    content=json_content,
                                    response_time=time.time() - start_time,
                                )
                            except json.JSONDecodeError as e:
                                error_msg = f"Invalid JSON: {str(e)}"
                                if attempt == self.max_retries:
                                    return FetchResult(
                                        url=url,
                                        success=False,
                                        error=error_msg,
                                        response_time=time.time() - start_time,
                                    )
                        else:
                            error_msg = f"HTTP {response.status}"
                            if attempt == self.max_retries:
                                return FetchResult(
                                    url=url,
                                    success=False,
                                    error=error_msg,
                                    response_time=time.time() - start_time,
                                )

            except asyncio.TimeoutError:
                error_msg = "Timeout"
                if attempt == self.max_retries:
                    return FetchResult(
                        url=url,
                        success=False,
                        error=error_msg,
                        response_time=time.time() - start_time,
                    )

            except aiohttp.ClientError as e:
                error_msg = f"Client error: {str(e)}"
                if attempt == self.max_retries:
                    return FetchResult(
                        url=url,
                        success=False,
                        error=error_msg,
                        response_time=time.time() - start_time,
                    )

            except Exception as e:
                error_msg = f"Unexpected error: {str(e)}"
                if attempt == self.max_retries:
                    return FetchResult(
                        url=url,
                        success=False,
                        error=error_msg,
                        response_time=time.time() - start_time,
                    )

            # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if attempt < self.max_retries:
                await asyncio.sleep(1 * (attempt + 1))

        # –≠—Ç–æ—Ç –∫–æ–¥ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è, –Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:
        return FetchResult(
            url=url,
            success=False,
            error="Max retries exceeded",
            response_time=time.time() - start_time,
        )

    async def fetch_urls(
        self, urls: List[str], output_file: str = "result.jsonl"
    ) -> Dict[str, Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª

        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            output_file: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        """
        stats = {"total": len(urls), "successful": 0, "failed": 0, "total_time": 0.0}

        start_time = time.time()

        async with self._create_session() as session:
            tasks = [self._fetch_single_url(session, url) for url in urls]

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è
            with open(output_file, "w", encoding="utf-8") as f:
                for future in asyncio.as_completed(tasks):
                    result = await future

                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    if result.success and result.content is not None:
                        output_data = {"url": result.url, "content": result.content}
                        f.write(json.dumps(output_data, ensure_ascii=False) + "\n")
                        f.flush()  # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ URL
                        stats["successful"] += 1
                        logger.info(
                            f"‚úÖ Success: {result.url} ({result.response_time:.2f}s)"
                        )
                    else:
                        stats["failed"] += 1
                        logger.warning(f"‚ùå Failed: {result.url} - {result.error}")

        stats["total_time"] = time.time() - start_time
        return stats


def read_urls_from_file(file_path: str, limit: Optional[int] = None) -> List[str]:
    """
    –ß–∏—Ç–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ñ–∞–π–ª–∞ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å URL
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è —á—Ç–µ–Ω–∏—è (None - –≤—Å–µ)

    Returns:
        –°–ø–∏—Å–æ–∫ URL
    """
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f"File {file_path} not found")

    urls = []
    with open(file_path, "r", encoding="utf-8") as f:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
        lines = f.readlines()
        start_index = 1 if lines and lines[0].strip().lower() == "url" else 0

        for i, line in enumerate(lines[start_index:], start=1):
            if limit is not None and len(urls) >= limit:
                break

            url = line.strip()
            if url and not url.startswith(
                "#"
            ):  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                urls.append(url)

    return urls


async def fetch_urls_from_file(
    input_file: str = "src/6/1.csv",
    output_file: str = "src/6/result.jsonl",
    max_concurrent: int = 5,
    timeout: int = 10,
    max_retries: int = 2,
    limit: Optional[int] = None,
) -> Dict[str, Any]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ URL –∏–∑ —Ñ–∞–π–ª–∞

    Args:
        input_file: –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª —Å URL
        output_file: –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        timeout: –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    try:
        # –ß–∏—Ç–∞–µ–º URL –∏–∑ —Ñ–∞–π–ª–∞
        urls = read_urls_from_file(input_file, limit=limit)

        if not urls:
            logger.warning("No URLs found in input file")
            return {}

        limit_info = f" (limited to first {limit})" if limit else ""
        logger.info(f"Found {len(urls)} URLs to process{limit_info}")

        # –°–æ–∑–¥–∞–µ–º —Ñ–µ—Ç—á–µ—Ä —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        fetcher = URLFetcher(
            max_concurrent=max_concurrent,
            timeout=aiohttp.ClientTimeout(total=timeout),
            max_retries=max_retries,
        )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        stats = await fetcher.fetch_urls(urls, output_file)

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info("=" * 50)
        logger.info("üìä PROCESSING COMPLETE")
        logger.info(f"Total URLs processed: {stats['total']}")
        logger.info(f"Successful: {stats['successful']}")
        logger.info(f"Failed: {stats['failed']}")
        logger.info(
            f"Success rate: {(stats['successful'] / stats['total'] * 100):.1f}%"
        )
        logger.info(f"Total time: {stats['total_time']:.2f} seconds")
        logger.info(
            f"Avg time per URL: {stats['total_time'] / stats['total']:.2f} seconds"
        )
        logger.info(f"Results saved to: {output_file}")

        return stats

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    import argparse

    parser = argparse.ArgumentParser(description="Async URL fetcher")
    parser.add_argument(
        "--input", default="src/6/1.csv", help="Input CSV file with URLs"
    )
    parser.add_argument(
        "--output", default="src/6/result.jsonl", help="Output JSONL file for results"
    )
    parser.add_argument(
        "--limit", type=int, help="Limit number of URLs to process (for testing)"
    )
    parser.add_argument(
        "--concurrent", type=int, default=5, help="Maximum concurrent requests"
    )
    parser.add_argument(
        "--timeout", type=int, default=120, help="Request timeout in seconds"
    )
    parser.add_argument("--retries", type=int, default=2, help="Maximum retry attempts")

    args = parser.parse_args()

    await fetch_urls_from_file(
        input_file=args.input,
        output_file=args.output,
        max_concurrent=args.concurrent,
        timeout=args.timeout,
        max_retries=args.retries,
        limit=args.limit,
    )


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    asyncio.run(main())
